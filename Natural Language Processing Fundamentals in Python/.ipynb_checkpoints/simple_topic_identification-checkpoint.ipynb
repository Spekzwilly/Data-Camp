{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = open('article.txt', 'r').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 147), ('of', 81), ('.', 70), ('to', 61), ('a', 59), (\"''\", 42), ('and', 41), ('in', 41), ('(', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter((lower_tokens))\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "english_stops = np.array(['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his',\n",
    " 'himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom',\n",
    " 'this', 'that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a',\n",
    " 'an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through',\n",
    " 'during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once',\n",
    " 'here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only',\n",
    " 'own','same','so','than','too','very','s','t','can','will','just','don','should','now','d','ll','m','o','re','ve','y','ain','aren',\n",
    " 'couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan','shouldn','wasn','weren','won' 'wouldn',\n",
    " ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\weiti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[('debugging', 30), ('system', 23), ('software', 16), ('computer', 14), ('bug', 14), ('problem', 14), ('term', 13), ('tool', 13), ('process', 12), ('used', 12)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
